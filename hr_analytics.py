# -*- coding: utf-8 -*-
"""HR_Analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ovZVzl-RBgdP5yGGNTquuu-Ruxi4ZUbJ
"""

from google.colab import drive
drive.mount('/content/gdrive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns



from sklearn.preprocessing import StandardScaler 
from sklearn.model_selection import train_test_split
#Decision Tree Clssification
from sklearn.tree import DecisionTreeClassifier

#Evaluation metrics
from sklearn.metrics import accuracy_score,confusion_matrix,roc_curve,roc_auc_score
from sklearn.metrics import classification_report
from sklearn import metrics

#Gradientboosting classifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import make_scorer

filepath="/content/gdrive/My Drive/hr_compition/train.csv"
hr_test=pd.read_csv("/content/gdrive/My Drive/hr_compition/test.csv")

def load_data(filepath):
    import pandas as pd
    import numpy as np
    data=pd.read_csv(filepath)
    return data

train_data=load_data(filepath)

cols=train_data.select_dtypes('object').copy()
cols

lis = []
for i in range(0, train_data.shape[1]):
    #print(i)
    if(train_data.iloc[:,i].dtypes == 'object'):
        train_data.iloc[:,i] = pd.Categorical(train_data.iloc[:,i])
        #print(marketing_train[[i]])
        train_data.iloc[:,i] = train_data.iloc[:,i].cat.codes 
        train_data.iloc[:,i] = train_data.iloc[:,i].astype('object')
        
        lis.append(train_data.columns[i])

#missingvalues_analysis
def mis_val(data):
    mis_val=data.isnull().sum()
    return  mis_val

mis_val(train_data)

#imputaion


train_data.previous_year_rating=train_data.previous_year_rating.fillna(method='backfill')



train_data.is_promoted=train_data.is_promoted.astype('object')
cat_columns=['department', 'region', 'education', 'gender', 'recruitment_channel','is_promoted']
cat_data=train_data[cat_columns]

num_data=train_data[train_data.columns[~train_data.columns.isin(['department', 'region', 'education', 'gender', 'recruitment_channel','is_promoted'])]]

"""**Outlier Analysis**"""

num_cols=num_data.columns

for i in num_cols:
   q75,q25=np.percentile(num_data.loc[:,i],[75,25])
   iqr=q75-q25
   min=q25-(iqr*1.5)
   max=q75+(iqr*1.5)
   print(i)
   print(min)
   print(max)
   num_data.loc[num_data[i]<min,i]=np.nan
   num_data.loc[num_data[i]>max,i]=np.nan

import matplotlib.pyplot as plt
plt.hist(num_data.avg_training_score)

mis_val(num_data)

from sklearn.impute import SimpleImputer
imputer=SimpleImputer(strategy='mean',missing_values=np.nan)
num_data=pd.DataFrame(imputer.fit_transform(num_data),columns=num_cols)

#FINDING highly correlated variables
def corr_matrix(data):
  #extract numeric data
  num_data=data.select_dtypes('float64').copy()
  # Create correlation matrix
  corr_matrix = num_data.corr().abs()
  # Select upper triangle of correlation matrix
  corr_mat= corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
  upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
  # Find index of feature columns with correlation greater than 0.95
  to_drop = [column for column in num_data.columns if any(upper[column] > 0.95)]
  sns.heatmap(corr_mat,cmap="Spectral")
 
  return num_data

num_data=corr_matrix(num_data)

cat_data.drop('is_promoted',axis=1,inplace=True)

#Chi square test 
from scipy.stats import chi2_contingency
cat_col=cat_data.columns
for i in cat_col:
    
    chi2,p,dof,ex=chi2_contingency(pd.crosstab(train_data['is_promoted'],cat_data[i]))
    print(float(p))

num_data.drop('employee_id',axis=1,inplace=True)

num_cols=num_data.columns

"""**Feature Scaling**"""



def feature_scaling(num_data):
  from sklearn.preprocessing import StandardScaler
  sc=StandardScaler()
  num_data_scaled=sc.fit_transform(num_data)
  return num_data_scaled

num_data_scaled=pd.DataFrame(feature_scaling(num_data_scaled),columns=num_cols)

X=pd.concat([num_data_scaled,cat_data],axis=1)
y=train_data['is_promoted']

"""**Train-test DataSplit**"""

def train_test_slpit(X,y):
   from sklearn.model_selection import train_test_slpit
   X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)
   return X_train,X_test,y_train,y_test

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y)

"""**DecisionTree Classifier**"""

#Decision Tree Clssification
from sklearn.tree import DecisionTreeClassifier

def Dec_tree(X_train,y_train):
  dc_tree=DecisionTreeClassifier()
  dc_tree.fit(X_train,y_train)
  y_predict=dc_tree.predict(X_test)
  return y_predict

#predicting probabilities

def metrics_classification(y_test,y_predict):
  cm=confusion_matrix(y_test,y_predict)
  probs=dc_tree.predict_proba(X_test)
  predcs=probs[:,1]
  fpr,tpr,threshold=metrics.roc_curve(y_test,predcs)
  roc_auc=metrics.auc(fpr,tpr)
  print (classification_report(y_test,y_predict))
  plt.title('Receiver Operating Characteristic')
  plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
  plt.legend(loc = 'lower right')
  plt.plot([0, 1], [0, 1],'r--')
  plt.xlim([0, 1])
  plt.ylim([0, 1])
  plt.ylabel('True Positive Rate')
  plt.xlabel('False Positive Rate')
  plt.show()
  return  classification_report(y_test,y_predict),fpr,tpr

y_pre_dc=Dec_tree(X_train,y_train)

"""# **LogisticRegression Model**"""

from sklearn.linear_model import LogisticRegression
def logit_model(X_train,y_train):
  lr=LogisticRegression()
  lr.fit(X_train,y_train)
  y_pre_lr=lr.predict(X_test)
  return y_pre_dc

import pickle
# Saving model to disk
pickle.dump(lr, open('model.pkl','wb'))



y_pre_lr=logit_model(X_train,y_train)
metrics_classification(y_test,y_pre_lr)





metrics_classification(y_test,y_predict)